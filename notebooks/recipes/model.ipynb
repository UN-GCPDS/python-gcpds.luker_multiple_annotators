{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd3756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# softmax_gp_composition_svgp.py\n",
    "# Python >= 3.9; pip install numpy pandas scikit-learn tensorflow>=2.10 gpflow>=2.9\n",
    "# This script:\n",
    "#   1) loads sensory and ingredient CSVs (joined by 'sample_id'),\n",
    "#   2) adds an 'Other' ingredient to preserve totals,\n",
    "#   3) standardizes sensory inputs,\n",
    "#   4) fits a Softmax-GP (SVGP multi-output) with a Monte-Carlo variational expectation,\n",
    "#   5) predicts valid compositions (sum to 1, nonnegative),\n",
    "#   6) (optional) evaluates Aitchison distance via CLR.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.kernels import RBF\n",
    "from gpflow.inducing_variables import InducingPoints\n",
    "from gpflow.kernels.multioutput import SeparateIndependent, SharedIndependent\n",
    "gpflow.config.set_default_float(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ba2df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utilities (simplex, CLR, metrics)\n",
    "# -----------------------------\n",
    "def to_simplex(rows: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    rows = np.clip(rows, eps, None)\n",
    "    rows = rows / rows.sum(axis=1, keepdims=True)\n",
    "    return rows\n",
    "\n",
    "def clr(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.clip(x, eps, None)\n",
    "    g = np.exp(np.mean(np.log(x), axis=1, keepdims=True))\n",
    "    return np.log(x / g)\n",
    "\n",
    "def aitchison_distance(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    C1 = clr(y_true, eps)\n",
    "    C2 = clr(y_pred, eps)\n",
    "    return np.sqrt(np.sum((C1 - C2) ** 2, axis=1))\n",
    "\n",
    "def softmax_tf(z, axis=-1):\n",
    "    zmax = tf.reduce_max(z, axis=axis, keepdims=True)\n",
    "    ez = tf.exp(z - zmax)\n",
    "    return ez / tf.reduce_sum(ez, axis=axis, keepdims=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Data handling\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class DataSpec:\n",
    "    sensory_csv: str\n",
    "    ingredients_csv: str\n",
    "    id_col: str = \"sample_id\"\n",
    "    add_other: bool = True\n",
    "    min_presence: int = 1     # keep an ingredient if present >= this many samples\n",
    "    scale_X: bool = True\n",
    "\n",
    "def load_and_align(spec: DataSpec) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    Xdf = pd.read_csv(spec.sensory_csv)\n",
    "    Ydf = pd.read_csv(spec.ingredients_csv)\n",
    "    df = Xdf.merge(Ydf, on=spec.id_col, how=\"inner\")\n",
    "    sensory_cols = [c for c in Xdf.columns if c != spec.id_col]\n",
    "    ingredient_cols = [c for c in Ydf.columns if c != spec.id_col]\n",
    "    X = df[[spec.id_col] + sensory_cols].copy()\n",
    "    Y = df[[spec.id_col] + ingredient_cols].copy()\n",
    "    return X, Y\n",
    "\n",
    "def build_composition(Y: pd.DataFrame, spec: DataSpec) -> Tuple[np.ndarray, List[str]]:\n",
    "    id_col = spec.id_col\n",
    "    cols = [c for c in Y.columns if c != id_col]\n",
    "    Yvals = Y[cols].to_numpy(dtype=float)\n",
    "\n",
    "    # If given in 0..100, normalize to 0..1 by row sum\n",
    "    row_sums = Yvals.sum(axis=1, keepdims=True)\n",
    "    row_sums = np.where(row_sums == 0.0, 1.0, row_sums)\n",
    "    Yvals = Yvals / row_sums\n",
    "\n",
    "    # Presence filter\n",
    "    present_counts = (Yvals > 0).sum(axis=0)\n",
    "    keep_mask = present_counts >= spec.min_presence\n",
    "    kept_cols_raw = [c for c, m in zip(cols, keep_mask) if m]\n",
    "    Y_keep = Yvals[:, keep_mask]\n",
    "\n",
    "    if spec.add_other:\n",
    "        leftover = 1.0 - Y_keep.sum(axis=1)\n",
    "        other = np.clip(leftover, 0.0, 1.0)\n",
    "        Y_aug = np.concatenate([Y_keep, other[:, None]], axis=1)\n",
    "        kept_cols = kept_cols_raw + [\"Other\"]\n",
    "    else:\n",
    "        # Renormalize kept subset\n",
    "        s = Y_keep.sum(axis=1, keepdims=True)\n",
    "        s = np.where(s == 0.0, 1.0, s)\n",
    "        Y_aug = Y_keep / s\n",
    "        kept_cols = kept_cols_raw\n",
    "\n",
    "    Y_aug = to_simplex(Y_aug)\n",
    "    return Y_aug, kept_cols\n",
    "\n",
    "# -----------------------------\n",
    "# Softmax-GP custom \"likelihood\"\n",
    "# -----------------------------\n",
    "class SoftmaxCompositional(gpflow.likelihoods.Likelihood):\n",
    "    \"\"\"\n",
    "    Softmax-based compositional likelihood:\n",
    "      log p(y | f) = tau * sum_k y_k * log softmax(f)_k\n",
    "    Vector-valued: latent_dim = observation_dim = input_dim = D.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D: int, tau: float = 100.0, eps: float = 1e-8):\n",
    "        # GPflow 2.9+ requires dims\n",
    "        super().__init__(input_dim=D, latent_dim=D, observation_dim=D)\n",
    "        self.D = int(D)\n",
    "        self.tau = tf.convert_to_tensor(tau, dtype=gpflow.default_float())\n",
    "        self.eps = tf.convert_to_tensor(eps, dtype=gpflow.default_float())\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _softmax(z, axis=-1):\n",
    "        zmax = tf.reduce_max(z, axis=axis, keepdims=True)\n",
    "        ez = tf.exp(z - zmax)\n",
    "        return ez / tf.reduce_sum(ez, axis=axis, keepdims=True)\n",
    "\n",
    "    def _loglik_point(self, F, Y):\n",
    "        # F, Y: (..., D)\n",
    "        P = tf.clip_by_value(self._softmax(F), self.eps, 1.0)\n",
    "        return self.tau * tf.reduce_sum(Y * tf.math.log(P), axis=-1)  # (...,)\n",
    "\n",
    "    # ---------- required by GPflow Likelihood ----------\n",
    "    def _log_prob(self, F, Y):\n",
    "        return self._loglik_point(F, Y)\n",
    "\n",
    "    def _variational_expectations(self, Fmu, Fvar, Y):\n",
    "        # Monte-Carlo VE: E_q[f][ log p(Y|f) ]\n",
    "        num_mc = 8\n",
    "        eps = tf.random.normal(shape=(num_mc,) + tf.shape(Fmu), dtype=Fmu.dtype)\n",
    "        F_sample = Fmu[None, ...] + tf.sqrt(Fvar[None, ...] + 1e-12) * eps  # (S,N,D)\n",
    "        logp = self._loglik_point(F_sample, Y[None, ...])                  # (S,N)\n",
    "        return tf.reduce_mean(logp, axis=0)                                 # (N,)\n",
    "\n",
    "    def _predict_mean_and_var(self, Fmu, Fvar):\n",
    "        # MC predictive moments in observation space\n",
    "        num_mc = 64\n",
    "        eps = tf.random.normal(shape=(num_mc,) + tf.shape(Fmu), dtype=Fmu.dtype)\n",
    "        F_sample = Fmu[None, ...] + tf.sqrt(Fvar[None, ...] + 1e-12) * eps  # (S,N,D)\n",
    "        P = self._softmax(F_sample, axis=-1)                                 # (S,N,D)\n",
    "        mean = tf.reduce_mean(P, axis=0)                                     # (N,D)\n",
    "        var = tf.math.reduce_variance(P, axis=0)                             # (N,D)\n",
    "        # ensure simplex numerically\n",
    "        mean = tf.clip_by_value(mean, self.eps, 1.0)\n",
    "        mean = mean / tf.reduce_sum(mean, axis=-1, keepdims=True)\n",
    "        return mean, var\n",
    "\n",
    "    def _predict_log_density(self, Fmu, Fvar, Y):\n",
    "        num_mc = 32\n",
    "        eps = tf.random.normal(shape=(num_mc,) + tf.shape(Fmu), dtype=Fmu.dtype)\n",
    "        F_sample = Fmu[None, ...] + tf.sqrt(Fvar[None, ...] + 1e-12) * eps\n",
    "        logp = self._loglik_point(F_sample, Y[None, ...])  # (S,N)\n",
    "        return tf.reduce_mean(logp, axis=0)                 # (N,)\n",
    "\n",
    "    # convenience used by your predict() wrapper\n",
    "    def predictive_mean_from_moments(self, Fmu, Fvar, mc: int = 64):\n",
    "        eps = tf.random.normal(shape=(mc,) + tf.shape(Fmu), dtype=Fmu.dtype)\n",
    "        F_sample = Fmu[None, ...] + tf.sqrt(Fvar[None, ...] + 1e-12) * eps\n",
    "        P = self._softmax(F_sample, axis=-1)\n",
    "        mean = tf.reduce_mean(P, axis=0)\n",
    "        mean = tf.clip_by_value(mean, self.eps, 1.0)\n",
    "        mean = mean / tf.reduce_sum(mean, axis=-1, keepdims=True)\n",
    "        return mean\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build & train the SVGP\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    num_inducing: int = 20\n",
    "    ard: bool = True\n",
    "    tau: float = 100.0\n",
    "    mc_pred: int = 64\n",
    "    max_iters: int = 5000\n",
    "    lr: float = 0.01\n",
    "    seed: int = 0\n",
    "\n",
    "def build_svgp_softmax(X: np.ndarray, Y: np.ndarray, cfg: ModelConfig):\n",
    "    tf.random.set_seed(cfg.seed)\n",
    "    N, P = X.shape\n",
    "    D = Y.shape[1]\n",
    "\n",
    "    M = min(cfg.num_inducing, N)\n",
    "    perm = np.random.RandomState(cfg.seed).permutation(N)[:M]\n",
    "    Z = X[perm, :].copy()\n",
    "\n",
    "    base_kern = RBF(lengthscales=np.ones(P), variance=1.0)  # ARD via vector lengthscales\n",
    "    kern = SharedIndependent(base_kern, output_dim=D)\n",
    "\n",
    "    lik = SoftmaxCompositional(D=D, tau=cfg.tau)  # <-- pass D here\n",
    "\n",
    "    inducing = InducingPoints(Z.astype(np.float64))\n",
    "    model = gpflow.models.SVGP(\n",
    "        kernel=kern,\n",
    "        likelihood=lik,\n",
    "        inducing_variable=inducing,\n",
    "        num_latent_gps=D,\n",
    "        q_diag=True,\n",
    "        whiten=True,\n",
    "    )\n",
    "\n",
    "    # small init for q_var improves stability\n",
    "    # Zero mean\n",
    "    model.q_mu.assign(tf.zeros_like(model.q_mu))\n",
    "\n",
    "    # q_sqrt shape = (L, M) when q_diag=True\n",
    "    model.q_sqrt.assign(1e-3 * tf.ones_like(model.q_sqrt))\n",
    "\n",
    "    return model, lik, Z\n",
    "\n",
    "\n",
    "\n",
    "def train_svgp(model, X, Y, cfg: ModelConfig):\n",
    "    \"\"\"\n",
    "    Adam on the ELBO (negative objective in gpflow is -ELBO).\n",
    "    \"\"\"\n",
    "    Xtf = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "    Ytf = tf.convert_to_tensor(Y, dtype=tf.float64)\n",
    "\n",
    "    opt = tf.optimizers.Adam(learning_rate=cfg.lr)\n",
    "\n",
    "    @tf.function(autograph=False)\n",
    "    def step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            elbo = model.elbo((Xtf, Ytf))\n",
    "            loss = -elbo\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return elbo\n",
    "\n",
    "    for it in range(cfg.max_iters):\n",
    "        elbo_val = step()\n",
    "        if (it + 1) % 500 == 0:\n",
    "            tf.print(\"iter\", it + 1, \"ELBO\", elbo_val)\n",
    "\n",
    "# -----------------------------\n",
    "# Predictions\n",
    "# -----------------------------\n",
    "def predict_composition(model, Xnew: np.ndarray, mc_samples: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns predictive mean composition (N*, D) by MC over q(f).\n",
    "    \"\"\"\n",
    "    Xtf = tf.convert_to_tensor(Xnew, dtype=tf.float64)\n",
    "    Fmu, Fvar = model.predict_f(Xtf, full_cov=False, full_output_cov=False)\n",
    "    lik: SoftmaxCompositional = model.likelihood  # type: ignore\n",
    "    Pmean = lik.predictive_mean_from_moments(Fmu, Fvar, mc=mc_samples).numpy()\n",
    "    # Ensure valid simplex numerically\n",
    "    Pmean = to_simplex(Pmean)\n",
    "    return Pmean\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end pipeline\n",
    "# -----------------------------\n",
    "def run_pipeline(\n",
    "    sensory_csv: str,\n",
    "    ingredients_csv: str,\n",
    "    id_col: str = \"sample_id\",\n",
    "    min_presence: int = 1,\n",
    "    add_other: bool = True,\n",
    "    num_inducing: int = 20,\n",
    "    tau: float = 100.0,\n",
    "    max_iters: int = 5000,\n",
    "    do_loocv: bool = True,\n",
    "    seed: int = 0\n",
    ") -> Dict:\n",
    "    spec = DataSpec(\n",
    "        sensory_csv=sensory_csv,\n",
    "        ingredients_csv=ingredients_csv,\n",
    "        id_col=id_col,\n",
    "        add_other=add_other,\n",
    "        min_presence=min_presence,\n",
    "        scale_X=True\n",
    "    )\n",
    "\n",
    "    # Load\n",
    "    Xdf, Ydf = load_and_align(spec)\n",
    "    meds = Xdf.iloc[:,1:].median(axis=0)\n",
    "    Xdf = Xdf.fillna(meds)\n",
    "    X_cols = [c for c in Xdf.columns if c != id_col]\n",
    "    X_raw = Xdf[X_cols].to_numpy(dtype=float)\n",
    "    Y_mat, ing_names = build_composition(Ydf, spec)  # (N, D)\n",
    "\n",
    "    # Standardize X\n",
    "    xscaler = StandardScaler().fit(X_raw)\n",
    "    X = xscaler.transform(X_raw)\n",
    "\n",
    "    # Build + train model\n",
    "    cfg = ModelConfig(\n",
    "        num_inducing=num_inducing, tau=tau, max_iters=max_iters, seed=seed, lr=0.01, ard=True\n",
    "    )\n",
    "    model, lik, Z = build_svgp_softmax(X, Y_mat, cfg)\n",
    "    train_svgp(model, X, Y_mat, cfg)\n",
    "\n",
    "    artifacts = {\n",
    "        \"xscaler\": xscaler,\n",
    "        \"model\": model,\n",
    "        \"ingredient_names\": ing_names,\n",
    "        \"X_cols\": X_cols,\n",
    "        \"id_col\": id_col,\n",
    "        \"config\": cfg.__dict__,\n",
    "    }\n",
    "\n",
    "    # Optional LOOCV (or k-fold if you prefer)\n",
    "    if do_loocv:\n",
    "        N = X.shape[0]\n",
    "        kf = KFold(n_splits=N, shuffle=True, random_state=seed)\n",
    "        ad_all, l2_all = [], []\n",
    "        for tr_idx, te_idx in kf.split(X):\n",
    "            Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "            Ytr, Yte = Y_mat[tr_idx], Y_mat[te_idx]\n",
    "\n",
    "            cfg_cv = ModelConfig(\n",
    "                num_inducing=min(cfg.num_inducing, Xtr.shape[0]),\n",
    "                tau=cfg.tau, max_iters=int(max_iters/2), seed=seed, lr=cfg.lr, ard=True\n",
    "            )\n",
    "            m_cv, _, _ = build_svgp_softmax(Xtr, Ytr, cfg_cv)\n",
    "            train_svgp(m_cv, Xtr, Ytr, cfg_cv)\n",
    "\n",
    "            Yhat = predict_composition(m_cv, Xte, mc_samples=cfg.mc_pred)\n",
    "            ad = aitchison_distance(Yte, Yhat)\n",
    "            l2 = np.sqrt(np.sum((Yte - Yhat) ** 2, axis=1))\n",
    "            ad_all.extend(list(ad))\n",
    "            l2_all.extend(list(l2))\n",
    "\n",
    "        artifacts[\"cv\"] = {\n",
    "            \"scheme\": \"LOOCV\",\n",
    "            \"aitchison_mean\": float(np.mean(ad_all)),\n",
    "            \"aitchison_std\": float(np.std(ad_all)),\n",
    "            \"l2_mean\": float(np.mean(l2_all)),\n",
    "            \"l2_std\": float(np.std(l2_all)),\n",
    "        }\n",
    "\n",
    "    return artifacts\n",
    "\n",
    "def predict_from_artifacts(artifacts: Dict, X_new: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_cols = artifacts[\"X_cols\"]\n",
    "    xscaler = artifacts[\"xscaler\"]\n",
    "    model = artifacts[\"model\"]\n",
    "    ing_names = artifacts[\"ingredient_names\"]\n",
    "\n",
    "    Xarr = X_new[X_cols].to_numpy(dtype=float)\n",
    "    Xs = xscaler.transform(Xarr)\n",
    "    Yhat = predict_composition(model, Xs, mc_samples=artifacts[\"config\"].get(\"mc_pred\", 64))\n",
    "    Yhat_pct = 100 * Yhat\n",
    "    return pd.DataFrame(Yhat_pct, columns=ing_names, index=X_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdc1b7fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SoftmaxCompositional._variational_expectations() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m SENSORY_CSV = \u001b[33m\"\u001b[39m\u001b[33m../../data/recipes/data_sens.csv\u001b[39m\u001b[33m\"\u001b[39m      \u001b[38;5;66;03m# columns: sample_id, s1..s8\u001b[39;00m\n\u001b[32m      6\u001b[39m ING_CSV     = \u001b[33m\"\u001b[39m\u001b[33m../../data/recipes/data_recipe.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# columns: sample_id, many ingredients (0..100 or 0..1)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m artifacts = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msensory_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSENSORY_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mingredients_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mING_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43midx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_presence\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# keep all ingredients as separate cols (rare ones flow into 'Other')\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_other\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inducing\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# try 50..300; larger -> sharper fits to observed compositions\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_loocv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m artifacts:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLOOCV Aitchison mean±std:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m             artifacts[\u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maitchison_mean\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m±\u001b[39m\u001b[33m\"\u001b[39m, artifacts[\u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maitchison_std\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 276\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(sensory_csv, ingredients_csv, id_col, min_presence, add_other, num_inducing, tau, max_iters, do_loocv, seed)\u001b[39m\n\u001b[32m    272\u001b[39m cfg = ModelConfig(\n\u001b[32m    273\u001b[39m     num_inducing=num_inducing, tau=tau, max_iters=max_iters, seed=seed, lr=\u001b[32m0.01\u001b[39m, ard=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    274\u001b[39m )\n\u001b[32m    275\u001b[39m model, lik, Z = build_svgp_softmax(X, Y_mat, cfg)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mtrain_svgp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m artifacts = {\n\u001b[32m    279\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mxscaler\u001b[39m\u001b[33m\"\u001b[39m: xscaler,\n\u001b[32m    280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: cfg.\u001b[34m__dict__\u001b[39m,\n\u001b[32m    285\u001b[39m }\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# Optional LOOCV (or k-fold if you prefer)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mtrain_svgp\u001b[39m\u001b[34m(model, X, Y, cfg)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elbo\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.max_iters):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     elbo_val = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (it + \u001b[32m1\u001b[39m) % \u001b[32m500\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    218\u001b[39m         tf.print(\u001b[33m\"\u001b[39m\u001b[33miter\u001b[39m\u001b[33m\"\u001b[39m, it + \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mELBO\u001b[39m\u001b[33m\"\u001b[39m, elbo_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mtrain_svgp.<locals>.step\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;129m@tf\u001b[39m.function(autograph=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mstep\u001b[39m():\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m         elbo = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43melbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m         loss = -elbo\n\u001b[32m    211\u001b[39m     grads = tape.gradient(loss, model.trainable_variables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\check_shapes\\integration\\tf.py:76\u001b[39m, in \u001b[36minstall_tf_integration.<locals>.TfWrapperPostProcessor.on_wrap.<locals>.wrapped_method\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped_method\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\gpflow\\models\\svgp.py:174\u001b[39m, in \u001b[36mSVGP_deprecated.elbo\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    172\u001b[39m kl = \u001b[38;5;28mself\u001b[39m.prior_kl()\n\u001b[32m    173\u001b[39m f_mean, f_var = \u001b[38;5;28mself\u001b[39m.predict_f(X, full_cov=\u001b[38;5;28;01mFalse\u001b[39;00m, full_output_cov=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m var_exp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariational_expectations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m     num_data = tf.cast(\u001b[38;5;28mself\u001b[39m.num_data, kl.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\check_shapes\\integration\\tf.py:76\u001b[39m, in \u001b[36minstall_tf_integration.<locals>.TfWrapperPostProcessor.on_wrap.<locals>.wrapped_method\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped_method\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\python-gcpds.luker_multiple_annotators\\venv\\Lib\\site-packages\\gpflow\\likelihoods\\base.py:263\u001b[39m, in \u001b[36mLikelihood.variational_expectations\u001b[39m\u001b[34m(self, X, Fmu, Fvar, Y)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;129m@check_shapes\u001b[39m(\n\u001b[32m    230\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX: [broadcast batch..., input_dim]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFmu: [broadcast batch..., latent_dim]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m     \u001b[38;5;28mself\u001b[39m, X: TensorType, Fmu: TensorType, Fvar: TensorType, Y: TensorType\n\u001b[32m    238\u001b[39m ) -> tf.Tensor:\n\u001b[32m    239\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03m    Compute the expected log density of the data, given a Gaussian\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    distribution for the function values,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    261\u001b[39m \u001b[33;03m    :returns: expected log density of the data given q(F)\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variational_expectations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: SoftmaxCompositional._variational_expectations() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "\n",
    "SENSORY_CSV = \"../../data/recipes/data_sens.csv\"      # columns: sample_id, s1..s8\n",
    "ING_CSV     = \"../../data/recipes/data_recipe.csv\"  # columns: sample_id, many ingredients (0..100 or 0..1)\n",
    "\n",
    "artifacts = run_pipeline(\n",
    "    sensory_csv=SENSORY_CSV,\n",
    "    ingredients_csv=ING_CSV,\n",
    "    id_col=\"idx\",\n",
    "    min_presence=1,     # keep all ingredients as separate cols (rare ones flow into 'Other')\n",
    "    add_other=True,\n",
    "    num_inducing=20,\n",
    "    tau=150.0,          # try 50..300; larger -> sharper fits to observed compositions\n",
    "    max_iters=10,\n",
    "    do_loocv=True,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "if \"cv\" in artifacts:\n",
    "    print(\"LOOCV Aitchison mean±std:\",\n",
    "            artifacts[\"cv\"][\"aitchison_mean\"], \"±\", artifacts[\"cv\"][\"aitchison_std\"])\n",
    "    print(\"LOOCV L2 mean±std:\",\n",
    "            artifacts[\"cv\"][\"l2_mean\"], \"±\", artifacts[\"cv\"][\"l2_std\"])\n",
    "\n",
    "# Demo predictions on the same X (replace with your new sensory rows)\n",
    "Xdf, _ = load_and_align(DataSpec(SENSORY_CSV, ING_CSV))\n",
    "preds = predict_from_artifacts(artifacts, Xdf.drop(columns=[\"sample_id\"]))\n",
    "print(preds.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eebf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
