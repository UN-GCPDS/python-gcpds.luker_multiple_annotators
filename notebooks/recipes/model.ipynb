{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91cd3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_gp_composition_svgp.py\n",
    "# Python >= 3.9; pip install numpy pandas scikit-learn tensorflow>=2.10 gpflow>=2.9\n",
    "# This script:\n",
    "#   1) loads sensory and ingredient CSVs (joined by 'sample_id'),\n",
    "#   2) adds an 'Other' ingredient to preserve totals,\n",
    "#   3) standardizes sensory inputs,\n",
    "#   4) fits a Softmax-GP (SVGP multi-output) with a Monte-Carlo variational expectation,\n",
    "#   5) predicts valid compositions (sum to 1, nonnegative),\n",
    "#   6) (optional) evaluates Aitchison distance via CLR.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.kernels import RBF\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "from gpflow.kernels.multioutput import SeparateIndependent, SharedIndependent\n",
    "gpflow.config.set_default_float(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utilities (simplex, CLR, metrics)\n",
    "# -----------------------------\n",
    "def to_simplex(rows: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    rows = np.clip(rows, eps, None)\n",
    "    rows = rows / rows.sum(axis=1, keepdims=True)\n",
    "    return rows\n",
    "\n",
    "def clr(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.clip(x, eps, None)\n",
    "    g = np.exp(np.mean(np.log(x), axis=1, keepdims=True))\n",
    "    return np.log(x / g)\n",
    "\n",
    "def aitchison_distance(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    C1 = clr(y_true, eps)\n",
    "    C2 = clr(y_pred, eps)\n",
    "    return np.sqrt(np.sum((C1 - C2) ** 2, axis=1))\n",
    "\n",
    "def softmax_tf(z, axis=-1):\n",
    "    zmax = tf.reduce_max(z, axis=axis, keepdims=True)\n",
    "    ez = tf.exp(z - zmax)\n",
    "    return ez / tf.reduce_sum(ez, axis=axis, keepdims=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Data handling\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class DataSpec:\n",
    "    sensory_csv: str\n",
    "    ingredients_csv: str\n",
    "    id_col: str = \"idx\"\n",
    "    add_other: bool = True\n",
    "    min_presence: int = 1     # keep an ingredient if present >= this many samples\n",
    "    scale_X: bool = True\n",
    "\n",
    "def load_and_align(spec: DataSpec) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    Xdf = pd.read_csv(spec.sensory_csv)\n",
    "    Ydf = pd.read_csv(spec.ingredients_csv)\n",
    "    df = Xdf.merge(Ydf, on=spec.id_col, how=\"inner\")\n",
    "    sensory_cols = [c for c in Xdf.columns if c != spec.id_col]\n",
    "    ingredient_cols = [c for c in Ydf.columns if c != spec.id_col]\n",
    "    X = df[[spec.id_col] + sensory_cols].copy()\n",
    "    Y = df[[spec.id_col] + ingredient_cols].copy()\n",
    "    return X, Y\n",
    "\n",
    "def build_composition(Y: pd.DataFrame, spec: DataSpec) -> Tuple[np.ndarray, List[str]]:\n",
    "    id_col = spec.id_col\n",
    "    cols = [c for c in Y.columns if c != id_col]\n",
    "    Yvals = Y[cols].to_numpy(dtype=float)\n",
    "\n",
    "    # If given in 0..100, normalize to 0..1 by row sum\n",
    "    row_sums = Yvals.sum(axis=1, keepdims=True)\n",
    "    row_sums = np.where(row_sums == 0.0, 1.0, row_sums)\n",
    "    Yvals = Yvals / row_sums\n",
    "\n",
    "    # Presence filter\n",
    "    present_counts = (Yvals > 0).sum(axis=0)\n",
    "    keep_mask = present_counts >= spec.min_presence\n",
    "    kept_cols_raw = [c for c, m in zip(cols, keep_mask) if m]\n",
    "    Y_keep = Yvals[:, keep_mask]\n",
    "\n",
    "    if spec.add_other:\n",
    "        leftover = 1.0 - Y_keep.sum(axis=1)\n",
    "        other = np.clip(leftover, 0.0, 1.0)\n",
    "        Y_aug = np.concatenate([Y_keep, other[:, None]], axis=1)\n",
    "        kept_cols = kept_cols_raw + [\"Other\"]\n",
    "    else:\n",
    "        # Renormalize kept subset\n",
    "        s = Y_keep.sum(axis=1, keepdims=True)\n",
    "        s = np.where(s == 0.0, 1.0, s)\n",
    "        Y_aug = Y_keep / s\n",
    "        kept_cols = kept_cols_raw\n",
    "\n",
    "    Y_aug = to_simplex(Y_aug)\n",
    "    return Y_aug, kept_cols\n",
    "\n",
    "# -----------------------------\n",
    "# Softmax-GP custom \"likelihood\"\n",
    "# -----------------------------\n",
    "class SoftmaxCompositional(gpflow.likelihoods.Likelihood):\n",
    "    \"\"\"\n",
    "    Softmax-based compositional likelihood:\n",
    "      log p(y | f) = tau * sum_k y_k * log softmax(f)_k\n",
    "    Vector-valued: latent_dim = observation_dim = input_dim = D.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D: int, tau: float = 100.0, eps: float = 1e-8):\n",
    "        super().__init__(input_dim=D, latent_dim=D, observation_dim=D)\n",
    "        self.D = int(D)\n",
    "        self.tau = tf.convert_to_tensor(tau, dtype=gpflow.default_float())\n",
    "        self.eps = tf.convert_to_tensor(eps, dtype=gpflow.default_float())\n",
    "\n",
    "    # -------- helpers --------\n",
    "    @staticmethod\n",
    "    def _softmax(z, axis=-1):\n",
    "        zmax = tf.reduce_max(z, axis=axis, keepdims=True)\n",
    "        ez = tf.exp(z - zmax)\n",
    "        return ez / tf.reduce_sum(ez, axis=axis, keepdims=True)\n",
    "\n",
    "    def _loglik_point(self, F, Y):\n",
    "        P = tf.clip_by_value(self._softmax(F), self.eps, 1.0)\n",
    "        return self.tau * tf.reduce_sum(Y * tf.math.log(P), axis=-1)  # (...,)\n",
    "\n",
    "    def _mc_sample_f(self, Fmu, Fvar, num_mc: int):\n",
    "        # Build epsilon from Fvar to guarantee broadcast compatibility\n",
    "        eps_shape = tf.concat([[num_mc], tf.shape(Fvar)], axis=0)  # (S, N, D)\n",
    "        eps = tf.random.normal(shape=eps_shape, dtype=Fvar.dtype)\n",
    "        Fstd = tf.sqrt(tf.maximum(Fvar, 0.0))\n",
    "        return Fmu[None, ...] + Fstd[None, ...] * eps  # (S, N, D)\n",
    "    def _ensure_nd_and_cast(self, X, Fmu, Fvar, Y):\n",
    "        # Cast dtypes\n",
    "        Fmu = tf.cast(Fmu, gpflow.default_float())\n",
    "        Fvar = tf.cast(Fvar, gpflow.default_float())\n",
    "        Y   = tf.cast(Y,   gpflow.default_float())\n",
    "\n",
    "        # Ranks must be 2: (N, D)\n",
    "        tf.debugging.assert_rank(Fmu, 2, message=\"Fmu must be rank-2 [N,D]\")\n",
    "        tf.debugging.assert_rank(Fvar, 2, message=\"Fvar must be rank-2 [N,D]\")\n",
    "        tf.debugging.assert_rank(Y,   2, message=\"Y must be rank-2 [N,D]\")\n",
    "\n",
    "        # Match N and D\n",
    "        n_fmu = tf.shape(Fmu)[0]; d_fmu = tf.shape(Fmu)[1]\n",
    "        n_fvar= tf.shape(Fvar)[0]; d_fvar= tf.shape(Fvar)[1]\n",
    "        n_y   = tf.shape(Y)[0];    d_y   = tf.shape(Y)[1]\n",
    "\n",
    "        tf.debugging.assert_equal(n_fmu, n_fvar, message=\"N mismatch: Fmu vs Fvar\")\n",
    "        tf.debugging.assert_equal(d_fmu, d_fvar, message=\"D mismatch: Fmu vs Fvar\")\n",
    "        tf.debugging.assert_equal(n_fmu, n_y,    message=\"N mismatch: Fmu/Fvar vs Y\")\n",
    "        tf.debugging.assert_equal(d_fmu, d_y,    message=\"D mismatch: Fmu/Fvar vs Y\")\n",
    "\n",
    "        # One-time shape print (remove after debugging)\n",
    "        tf.print(\"[LIK] shapes -> Fmu\", tf.shape(Fmu), \"Fvar\", tf.shape(Fvar), \"Y\", tf.shape(Y), summarize=-1)\n",
    "        return Fmu, Fvar, Y\n",
    "\n",
    "    # -------- required by GPflow Likelihood (note the X arg) --------\n",
    "    def _log_prob(self, X, F, Y):\n",
    "        # X is unused, but required by the signature\n",
    "        return self._loglik_point(F, Y)\n",
    "\n",
    "    def _variational_expectations(self, X, Fmu, Fvar, Y):\n",
    "        F_sample = self._mc_sample_f(Fmu, Fvar, num_mc=8)\n",
    "        logp = self._loglik_point(F_sample, Y[None, ...])  # (S, N)\n",
    "        return tf.reduce_mean(logp, axis=0)\n",
    "\n",
    "    def _predict_mean_and_var(self, X, Fmu, Fvar):\n",
    "        F_sample = self._mc_sample_f(Fmu, Fvar, num_mc=64)\n",
    "        P = self._softmax(F_sample, axis=-1)              # (S, N, D)\n",
    "        mean = tf.reduce_mean(P, axis=0)                  # (N, D)\n",
    "        var  = tf.math.reduce_variance(P, axis=0)         # (N, D)\n",
    "        mean = tf.clip_by_value(mean, self.eps, 1.0)\n",
    "        mean = mean / tf.reduce_sum(mean, axis=-1, keepdims=True)\n",
    "        return mean, var\n",
    "\n",
    "    def _predict_log_density(self, X, Fmu, Fvar, Y):\n",
    "        F_sample = self._mc_sample_f(Fmu, Fvar, num_mc=32)\n",
    "        logp = self._loglik_point(F_sample, Y[None, ...])  # (S, N)\n",
    "        return tf.reduce_mean(logp, axis=0)\n",
    "\n",
    "    def predictive_mean_from_moments(self, Fmu, Fvar, mc: int = 64):\n",
    "        F_sample = self._mc_sample_f(Fmu, Fvar, num_mc=mc)\n",
    "        P = self._softmax(F_sample, axis=-1)\n",
    "        mean = tf.reduce_mean(P, axis=0)\n",
    "        mean = tf.clip_by_value(mean, self.eps, 1.0)\n",
    "        mean = mean / tf.reduce_sum(mean, axis=-1, keepdims=True)\n",
    "        return mean\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build & train the SVGP\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    num_inducing: int = 20\n",
    "    ard: bool = True\n",
    "    tau: float = 100.0\n",
    "    mc_pred: int = 64\n",
    "    max_iters: int = 5000\n",
    "    lr: float = 0.01\n",
    "    seed: int = 0\n",
    "\n",
    "def build_svgp_softmax(X: np.ndarray, Y: np.ndarray, cfg: ModelConfig):\n",
    "    tf.random.set_seed(cfg.seed)\n",
    "    N, P = X.shape\n",
    "    D = Y.shape[1]\n",
    "\n",
    "    M = min(cfg.num_inducing, N)\n",
    "    perm = np.random.RandomState(cfg.seed).permutation(N)[:M]\n",
    "    Z = X[perm, :].copy()\n",
    "\n",
    "    base_kern = RBF(lengthscales=np.ones(P), variance=1.0)  # ARD via vector lengthscales\n",
    "    kern = SharedIndependent(base_kern, output_dim=D)\n",
    "\n",
    "    lik = SoftmaxCompositional(D=D, tau=cfg.tau)  # <-- pass D here\n",
    "\n",
    "    inducing = SharedIndependentInducingVariables(InducingPoints(Z.astype(np.float64)))\n",
    "    model = gpflow.models.SVGP(\n",
    "        kernel=kern,\n",
    "        likelihood=lik,\n",
    "        inducing_variable=inducing,\n",
    "        num_latent_gps=D,\n",
    "        q_diag=True,\n",
    "        whiten=True,\n",
    "    )\n",
    "\n",
    "    # small init for q_var improves stability\n",
    "    # Zero mean\n",
    "    model.q_mu.assign(tf.zeros_like(model.q_mu))\n",
    "\n",
    "    # q_sqrt shape = (L, M) when q_diag=True\n",
    "    model.q_sqrt.assign(1e-3 * tf.ones_like(model.q_sqrt))\n",
    "\n",
    "    return model, lik, Z\n",
    "\n",
    "\n",
    "\n",
    "def train_svgp(\n",
    "    model,\n",
    "    X,\n",
    "    Y,\n",
    "    cfg: ModelConfig,\n",
    "    patience: int = 50,       # how many iterations to wait for improvement\n",
    "    min_delta: float = 1e-3,  # minimal ELBO gain to count as improvement\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train SVGP with Adam, monitoring ELBO and stopping early if it stalls.\n",
    "\n",
    "    Args:\n",
    "        model: gpflow SVGP\n",
    "        X, Y: training arrays\n",
    "        cfg: ModelConfig\n",
    "        patience: stop if no ELBO improvement for this many steps\n",
    "        min_delta: threshold for \"improvement\"\n",
    "        verbose: whether to print progress\n",
    "    \"\"\"\n",
    "    Xtf = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "    Ytf = tf.convert_to_tensor(Y, dtype=tf.float64)\n",
    "\n",
    "    opt = tf.optimizers.Adam(learning_rate=cfg.lr)\n",
    "\n",
    "    @tf.function(autograph=False)\n",
    "    def step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            elbo = model.elbo((Xtf, Ytf))\n",
    "            loss = -elbo\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return elbo\n",
    "\n",
    "    best_elbo = -np.inf\n",
    "    wait = 0\n",
    "\n",
    "    for it in range(cfg.max_iters):\n",
    "        elbo_val = step().numpy()\n",
    "\n",
    "        # Check improvement\n",
    "        if elbo_val > best_elbo + min_delta:\n",
    "            best_elbo = elbo_val\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        # Logging\n",
    "        if verbose and (it + 1) % 50 == 0:\n",
    "            print(f\"[train] iter {it+1:5d}, ELBO={elbo_val:.4f}, best={best_elbo:.4f}, wait={wait}\")\n",
    "\n",
    "        # Early stop\n",
    "        if wait >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[train] Early stopping at iter {it+1}, best ELBO={best_elbo:.4f}\")\n",
    "            break\n",
    "\n",
    "    return best_elbo\n",
    "\n",
    "# -----------------------------\n",
    "# Predictions\n",
    "# -----------------------------\n",
    "def predict_composition(model, Xnew: np.ndarray, mc_samples: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns predictive mean composition (N*, D) by MC over q(f).\n",
    "    \"\"\"\n",
    "    Xtf = tf.convert_to_tensor(Xnew, dtype=tf.float64)\n",
    "    Fmu, Fvar = model.predict_f(Xtf, full_cov=False, full_output_cov=False)\n",
    "    lik: SoftmaxCompositional = model.likelihood  # type: ignore\n",
    "    Pmean = lik.predictive_mean_from_moments(Fmu, Fvar, mc=mc_samples).numpy()\n",
    "    # Ensure valid simplex numerically\n",
    "    Pmean = to_simplex(Pmean)\n",
    "    return Pmean\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end pipeline\n",
    "# -----------------------------\n",
    "def run_pipeline(\n",
    "    sensory_csv: str,\n",
    "    ingredients_csv: str,\n",
    "    id_col: str = \"sample_id\",\n",
    "    min_presence: int = 1,\n",
    "    add_other: bool = True,\n",
    "    num_inducing: int = 20,\n",
    "    tau: float = 100.0,\n",
    "    max_iters: int = 5000,\n",
    "    do_loocv: bool = True,\n",
    "    seed: int = 0\n",
    ") -> Dict:\n",
    "    spec = DataSpec(\n",
    "        sensory_csv=sensory_csv,\n",
    "        ingredients_csv=ingredients_csv,\n",
    "        id_col=id_col,\n",
    "        add_other=add_other,\n",
    "        min_presence=min_presence,\n",
    "        scale_X=True\n",
    "    )\n",
    "\n",
    "    # Load\n",
    "    Xdf, Ydf = load_and_align(spec)\n",
    "    meds = Xdf.iloc[:,1:].median(axis=0)\n",
    "    Xdf = Xdf.fillna(meds)\n",
    "    Ydf = Ydf.fillna(0)\n",
    "    X_cols = [c for c in Xdf.columns if c != id_col]\n",
    "    X_raw = Xdf[X_cols].to_numpy(dtype=float)\n",
    "    Y_mat, ing_names = build_composition(Ydf, spec)  # (N, D)\n",
    "\n",
    "    # Standardize X\n",
    "    xscaler = StandardScaler().fit(X_raw)\n",
    "    X = xscaler.transform(X_raw)\n",
    "\n",
    "    # Build + train model\n",
    "    cfg = ModelConfig(\n",
    "        num_inducing=num_inducing, tau=tau, max_iters=max_iters, seed=seed, lr=0.01, ard=True\n",
    "    )\n",
    "    model, lik, Z = build_svgp_softmax(X, Y_mat, cfg)\n",
    "\n",
    "    train_svgp(model, X, Y_mat, cfg)\n",
    "\n",
    "    artifacts = {\n",
    "        \"xscaler\": xscaler,\n",
    "        \"x_imputer\": meds,          # <-- store the training medians\n",
    "        \"model\": model,\n",
    "        \"ingredient_names\": ing_names,\n",
    "        \"X_cols\": X_cols,\n",
    "        \"id_col\": id_col,\n",
    "        \"config\": cfg.__dict__,\n",
    "    }\n",
    "\n",
    "    n = X.shape[0]\n",
    "    run_cv = bool(do_loocv)\n",
    "    n_splits = None\n",
    "    scheme = None\n",
    "\n",
    "    if do_loocv is True:\n",
    "        n_splits = n\n",
    "        scheme = \"LOOCV\"\n",
    "    elif isinstance(do_loocv, int) and do_loocv >= 2:\n",
    "        n_splits = min(do_loocv, n)  # cap at N\n",
    "        scheme = f\"{n_splits}-Fold CV\"\n",
    "\n",
    "    if run_cv and n_splits is not None and n_splits >= 2:\n",
    "        # We will CV on *raw* X (before full-data scaling) to avoid leakage.\n",
    "        # Recreate raw X and fixed Y from the same columns used to train the full model.\n",
    "        X_raw_full = Xdf[X_cols].to_numpy(dtype=float)  # raw (post-imputation only for final model)\n",
    "        Y_full = Y_mat                                  # (N, D) fixed composition columns\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        ad_all, l2_all = [], []\n",
    "\n",
    "        for fold_idx, (tr_idx, te_idx) in enumerate(kf.split(X_raw_full), 1):\n",
    "            # --- Train split: imputer + scaler fit only on TRAIN ---\n",
    "            Xtr_raw = X_raw_full[tr_idx]\n",
    "            Xte_raw = X_raw_full[te_idx]\n",
    "\n",
    "            # Impute with training medians (column-wise)\n",
    "            meds_tr = np.nanmedian(Xtr_raw, axis=0)\n",
    "            # If a column is entirely NaN, replace nanmedian with 0\n",
    "            meds_tr = np.where(np.isfinite(meds_tr), meds_tr, 0.0)\n",
    "            Xtr_imp = np.where(np.isnan(Xtr_raw), meds_tr[None, :], Xtr_raw)\n",
    "            Xte_imp = np.where(np.isnan(Xte_raw), meds_tr[None, :], Xte_raw)\n",
    "\n",
    "            # Scale with training scaler\n",
    "            scaler_tr = StandardScaler().fit(Xtr_imp)\n",
    "            # Guard zero-variance columns\n",
    "            if hasattr(scaler_tr, \"scale_\"):\n",
    "                zero_scale = (scaler_tr.scale_ == 0)\n",
    "                if np.any(zero_scale):\n",
    "                    scaler_tr.scale_[zero_scale] = 1.0\n",
    "                    if hasattr(scaler_tr, \"var_\"):\n",
    "                        scaler_tr.var_[zero_scale] = 1.0\n",
    "\n",
    "            Xtr = scaler_tr.transform(Xtr_imp)\n",
    "            Xte = scaler_tr.transform(Xte_imp)\n",
    "\n",
    "            # Targets\n",
    "            Ytr, Yte = Y_full[tr_idx], Y_full[te_idx]\n",
    "\n",
    "            # Build & train fold model\n",
    "            cfg_cv = ModelConfig(\n",
    "                num_inducing=min(cfg.num_inducing, Xtr.shape[0]),\n",
    "                tau=cfg.tau,\n",
    "                max_iters=max(1, int(max_iters / 2)),   # faster per fold\n",
    "                seed=seed,\n",
    "                lr=cfg.lr,\n",
    "                ard=True\n",
    "            )\n",
    "            m_cv, _, _ = build_svgp_softmax(Xtr, Ytr, cfg_cv)\n",
    "            train_svgp(m_cv, Xtr, Ytr, cfg_cv)\n",
    "\n",
    "            # Predict on test split\n",
    "            Yhat = predict_composition(m_cv, Xte, mc_samples=cfg.mc_pred)\n",
    "\n",
    "            # Metrics\n",
    "            ad = aitchison_distance(Yte, Yhat)\n",
    "            l2 = np.sqrt(np.sum((Yte - Yhat) ** 2, axis=1))\n",
    "            ad_all.extend(list(ad))\n",
    "            l2_all.extend(list(l2))\n",
    "\n",
    "        artifacts[\"cv\"] = {\n",
    "            \"scheme\": scheme,\n",
    "            \"folds\": int(n_splits),\n",
    "            \"aitchison_mean\": float(np.mean(ad_all)),\n",
    "            \"aitchison_std\":  float(np.std(ad_all)),\n",
    "            \"l2_mean\":        float(np.mean(l2_all)),\n",
    "            \"l2_std\":         float(np.std(l2_all)),\n",
    "            \"n_samples\":      int(n),\n",
    "        }\n",
    "\n",
    "    return artifacts\n",
    "\n",
    "def predict_from_artifacts(artifacts: Dict, X_new: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_cols = artifacts[\"X_cols\"]\n",
    "    xscaler = artifacts[\"xscaler\"]\n",
    "    x_imputer = artifacts[\"x_imputer\"]      # <-- use the stored medians\n",
    "    model = artifacts[\"model\"]\n",
    "    ing_names = artifacts[\"ingredient_names\"]\n",
    "\n",
    "    # Make a copy, ensure order, impute\n",
    "    Xarr_df = X_new[X_cols].astype(float).copy()\n",
    "    Xarr_df = Xarr_df.fillna(x_imputer)\n",
    "\n",
    "    # (Optional) if any constant columns at train time, skip second check here\n",
    "    Xs = xscaler.transform(Xarr_df.values)\n",
    "\n",
    "    # Simple finite check (see Patch C)\n",
    "    assert np.isfinite(Xs).all(), \"Xs has non-finite values after impute+scale.\"\n",
    "\n",
    "    Yhat = predict_composition(model, Xs, mc_samples=artifacts[\"config\"].get(\"mc_pred\", 64))\n",
    "    Yhat_pct = 100 * Yhat\n",
    "    return pd.DataFrame(Yhat_pct, columns=ing_names, index=X_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fdc1b7fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'StandardScaler' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m SENSORY_CSV = \u001b[33m\"\u001b[39m\u001b[33m../../data/recipes/data_sens.csv\u001b[39m\u001b[33m\"\u001b[39m      \u001b[38;5;66;03m# columns: sample_id, s1..s8\u001b[39;00m\n\u001b[32m      6\u001b[39m ING_CSV     = \u001b[33m\"\u001b[39m\u001b[33m../../data/recipes/data_recipe.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# columns: sample_id, many ingredients (0..100 or 0..1)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m artifacts = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msensory_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSENSORY_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mingredients_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mING_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43midx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_presence\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# keep all ingredients as separate cols (rare ones flow into 'Other')\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_other\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inducing\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# try 50..300; larger -> sharper fits to observed compositions\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_loocv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m artifacts:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLOOCV Aitchison mean±std:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m             artifacts[\u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maitchison_mean\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m±\u001b[39m\u001b[33m\"\u001b[39m, artifacts[\u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maitchison_std\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 324\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(sensory_csv, ingredients_csv, id_col, min_presence, add_other, num_inducing, tau, max_iters, do_loocv, seed)\u001b[39m\n\u001b[32m    321\u001b[39m Y_mat, ing_names = build_composition(Ydf, spec)  \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# Standardize X\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m xscaler = \u001b[43mStandardScaler\u001b[49m().fit(X_raw)\n\u001b[32m    325\u001b[39m X = xscaler.transform(X_raw)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Build + train model\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'StandardScaler' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "\n",
    "SENSORY_CSV = \"../../data/recipes/data_sens.csv\"      # columns: sample_id, s1..s8\n",
    "ING_CSV     = \"../../data/recipes/data_recipe.csv\"  # columns: sample_id, many ingredients (0..100 or 0..1)\n",
    "\n",
    "artifacts = run_pipeline(\n",
    "    sensory_csv=SENSORY_CSV,\n",
    "    ingredients_csv=ING_CSV,\n",
    "    id_col=\"idx\",\n",
    "    min_presence=1,     # keep all ingredients as separate cols (rare ones flow into 'Other')\n",
    "    add_other=True,\n",
    "    num_inducing=20,\n",
    "    tau=150.0,          # try 50..300; larger -> sharper fits to observed compositions\n",
    "    max_iters=10,\n",
    "    do_loocv=True,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "if \"cv\" in artifacts:\n",
    "    print(\"LOOCV Aitchison mean±std:\",\n",
    "            artifacts[\"cv\"][\"aitchison_mean\"], \"±\", artifacts[\"cv\"][\"aitchison_std\"])\n",
    "    print(\"LOOCV L2 mean±std:\",\n",
    "            artifacts[\"cv\"][\"l2_mean\"], \"±\", artifacts[\"cv\"][\"l2_std\"])\n",
    "\n",
    "# Demo predictions on the same X (replace with your new sensory rows)\n",
    "Xdf, _ = load_and_align(DataSpec(SENSORY_CSV, ING_CSV))\n",
    "preds = predict_from_artifacts(artifacts, Xdf.drop(columns=[\"idx\"]))\n",
    "print(preds.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b2040",
   "metadata": {},
   "source": [
    "## debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0172a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_and_align_simple(\n",
    "    sensory_csv: str,\n",
    "    ingredients_csv: str,\n",
    "    id_col: str = \"sample_id\",\n",
    "    add_other: bool = True,\n",
    "    min_presence: int = 1,   # keep columns that appear in >= min_presence samples\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_df: imputed+z-scored sensory DataFrame [id_col + features]\n",
    "      Y_df: row-normalized composition DataFrame [id_col + ingredients_kept (+ Other)]\n",
    "      x_imputer: pd.Series of medians (for reuse at predict time)\n",
    "      x_scaler: fitted StandardScaler (for reuse at predict time)\n",
    "    \"\"\"\n",
    "    # --- Load & join ---\n",
    "    Xraw = pd.read_csv(sensory_csv)\n",
    "    Yraw = pd.read_csv(ingredients_csv)\n",
    "    df = Xraw.merge(Yraw, on=id_col, how=\"inner\")\n",
    "\n",
    "    # --- Identify cols ---\n",
    "    sensory_cols = [c for c in Xraw.columns if c != id_col]\n",
    "    ing_cols     = [c for c in Yraw.columns if c != id_col]\n",
    "\n",
    "    # --- X: impute (median) + z-score ---\n",
    "    X_sens = df[sensory_cols].astype(float)\n",
    "    x_imputer = X_sens.median(axis=0)                # robust to outliers / NaNs\n",
    "    X_imputed = X_sens.fillna(x_imputer)\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    X_scaled = x_scaler.fit_transform(X_imputed.values)\n",
    "\n",
    "    X_df = pd.DataFrame(X_scaled, columns=sensory_cols, index=df.index)\n",
    "    X_df.insert(0, id_col, df[id_col].values)\n",
    "\n",
    "    # --- Y: fillna->0, (optional) frequency filter, add 'Other', row-normalize to 1 ---\n",
    "    Y_mat = df[ing_cols].astype(float).fillna(0.0).to_numpy()\n",
    "\n",
    "    # frequency filter (keep ingredients appearing in >= min_presence samples)\n",
    "    present_counts = (Y_mat > 0).sum(axis=0)\n",
    "    keep_mask = present_counts >= int(min_presence)\n",
    "    kept_cols = [c for c, keep in zip(ing_cols, keep_mask) if keep]\n",
    "    Y_keep = Y_mat[:, keep_mask]\n",
    "\n",
    "    if add_other:\n",
    "        leftover = 1.0 - Y_keep.sum(axis=1, keepdims=True)\n",
    "        other = np.clip(leftover, 0.0, None)  # anything not explained by kept ingredients\n",
    "        Y_aug = np.concatenate([Y_keep, other], axis=1)\n",
    "        out_cols = kept_cols + [\"Other\"]\n",
    "    else:\n",
    "        Y_aug = Y_keep\n",
    "        out_cols = kept_cols\n",
    "\n",
    "    # row normalize to the simplex\n",
    "    row_sums = Y_aug.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    Y_comp = Y_aug / row_sums\n",
    "\n",
    "    Y_df = pd.DataFrame(Y_comp, columns=out_cols, index=df.index)\n",
    "    Y_df.insert(0, id_col, df[id_col].values)\n",
    "\n",
    "    # --- Sanity prints (once) ---\n",
    "    N, P = X_df.shape[0], len(sensory_cols)\n",
    "    D = len(out_cols)\n",
    "    print(f\"[load] N={N}, P={P} sensory; D={D} ingredients (incl. Other={add_other})\")\n",
    "    print(\"  • X: any NaN? ->\", pd.isna(X_df[sensory_cols]).any().any())\n",
    "    print(\"  • Y: any NaN? ->\", pd.isna(Y_df[out_cols]).any().any())\n",
    "    print(\"  • Y rows (first 3) sums:\", Y_df[out_cols].sum(axis=1).head(3).round(6).to_list())\n",
    "\n",
    "    return X_df, Y_df, x_imputer, x_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dafdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] N=93, P=8 sensory; D=75 ingredients (incl. Other=False)\n",
      "  • X: any NaN? -> False\n",
      "  • Y: any NaN? -> False\n",
      "  • Y rows (first 3) sums: [1.0, 1.0, 1.0]\n",
      "X_df shape: (93, 9)\n",
      "Y_df shape: (93, 76)\n"
     ]
    }
   ],
   "source": [
    "SENSORY_CSV = \"../../data/recipes/data_sens.csv\"      # has: idx, s1..s8\n",
    "ING_CSV     = \"../../data/recipes/data_recipe.csv\"    # has: idx, many ingredients\n",
    "\n",
    "X_df, Y_df, x_imputer, x_scaler = load_and_align_simple(\n",
    "    sensory_csv=SENSORY_CSV,\n",
    "    ingredients_csv=ING_CSV,\n",
    "    id_col=\"idx\",\n",
    "    add_other=False,\n",
    "    min_presence=1,   # keep everything; rare stuff goes to Other implicitly\n",
    ")\n",
    "\n",
    "# Check a few shapes\n",
    "id_col = \"idx\"\n",
    "sens_cols = [c for c in X_df.columns if c != id_col]\n",
    "ing_cols  = [c for c in Y_df.columns if c != id_col]\n",
    "print(\"X_df shape:\", X_df.shape)\n",
    "print(\"Y_df shape:\", Y_df.shape)\n",
    "assert np.allclose(Y_df[ing_cols].sum(axis=1).values, 1.0, atol=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5516251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILR Z shape: (93, 74)\n",
      "Round-trip max abs err: 7.399902912652578e-11\n",
      "Round-trip row sums (first 3): [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def helmert_basis(d: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Orthonormal Helmert sub-matrix (d x (d-1)).\n",
    "    \"\"\"\n",
    "    H = np.zeros((d, d-1))\n",
    "    for j in range(1, d):\n",
    "        e = np.ones(j) / j\n",
    "        v = np.concatenate([e, [-1.0], np.zeros(d - j - 1)])\n",
    "        H[:, j-1] = v\n",
    "    # scale columns to get orthonormal basis\n",
    "    for j in range(d - 1):\n",
    "        H[:, j] *= np.sqrt((j + 1) / (j + 2))\n",
    "    return H\n",
    "\n",
    "def to_simplex(A: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    A = np.clip(A, eps, None)\n",
    "    return A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "def clr(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    X = np.clip(X, eps, None)\n",
    "    g = np.exp(np.mean(np.log(X), axis=1, keepdims=True))\n",
    "    return np.log(X / g)\n",
    "\n",
    "def clr_inv(C: np.ndarray) -> np.ndarray:\n",
    "    Y = np.exp(C)\n",
    "    return Y / Y.sum(axis=1, keepdims=True)\n",
    "\n",
    "def ilr(X: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "    return clr(X) @ H\n",
    "\n",
    "def ilr_inv(Z: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "    return clr_inv(Z @ H.T)\n",
    "\n",
    "# ---- Round-trip test with your Y ----\n",
    "id_col = \"idx\"\n",
    "ing_cols = [c for c in Y_df.columns if c != id_col]\n",
    "Y = Y_df[ing_cols].to_numpy(dtype=float)\n",
    "D = Y.shape[1]\n",
    "H = helmert_basis(D)\n",
    "\n",
    "Z = ilr(Y, H)\n",
    "Y_back = ilr_inv(Z, H)\n",
    "\n",
    "print(\"ILR Z shape:\", Z.shape)     # (N, D-1)\n",
    "print(\"Round-trip max abs err:\", np.max(np.abs(Y - Y_back)))\n",
    "print(\"Round-trip row sums (first 3):\", Y_back.sum(axis=1)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f734159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
